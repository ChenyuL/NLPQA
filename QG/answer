#!/usr/bin/env python3

# pip3 install sentencepiece
# pip3 install spacy
# python3 -m spacy download en_core_web_sm
# pip3 install torch
# pip3 install transformers
# pip3 pip3 install --upgrade pip
# ! pip3 install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]

import logging
import sys

print(sys.argv[0])
import time
import os
from haystack.utils import clean_wiki_text, convert_files_to_docs
from haystack.nodes import FARMReader
from nltk.corpus import wordnet
import spacy

#
# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q
# tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz
# chown -R daemon:daemon elasticsearch-7.9.2

DEBUG = True
USE_LOCAL_MODELS = False
######## get semantic negation for similarity calculation
def Negation(sentence):
  '''
  Input: Tokenized sentence (List of words)
  Output: Tokenized sentence with negation handled (List of words)
  '''
  temp = int(0)
  for i in range(len(sentence)):
      if sentence[i-1] in ['not',"n't"]:
          antonyms = []
          for syn in wordnet.synsets(sentence[i]):
              syns = wordnet.synsets(sentence[i])
              w1 = syns[0].name()
              temp = 0
              for l in syn.lemmas():
                  if l.antonyms():
                      antonyms.append(l.antonyms()[0].name())
              max_dissimilarity = 0
              for ant in antonyms:
                  syns = wordnet.synsets(ant)
                  w2 = syns[0].name()
                  syns = wordnet.synsets(sentence[i])
                  w1 = syns[0].name()
                  word1 = wordnet.synset(w1)
                  word2 = wordnet.synset(w2)
                  if isinstance(word1.wup_similarity(word2), float) or isinstance(word1.wup_similarity(word2), int):
                      temp = 1 - word1.wup_similarity(word2)
                  if temp>max_dissimilarity:
                      max_dissimilarity = temp
                      antonym_max = ant
                      sentence[i] = antonym_max
                      sentence[i-1] = ''
  while '' in sentence:
      sentence.remove('')
  return sentence

def isYesNoQuestion(question):
    s = question.lower()
    prefixes = ['am','is','are',"was","were", "wasn't", "weren't",
                "do","does","did","doesn't","didn't",
                "has","have","hasn't", "haven't","had",
                "can","could","will","would","may","might","shall","must","should"] # reference https://englishstudypage.com/grammar/yesno-questions/
    result = s.startswith(tuple(prefixes))
    print(result)
    return result

def answerWHQuestion(question,file):

    logging.basicConfig(format="%(levelname)s - %(name)s -  %(message)s", level=logging.WARNING)
    logging.getLogger("haystack").setLevel(logging.INFO)


    #### set document folder

    ###### test
    # keep data as the document file
    # create a new directory called article_directory
    # put all the articles in the article_directory
    os.mkdir('article_directory')

    # add the argv[1] to the article_directory
    os.system('cp ' + file + ' article_directory')
 

    docs = convert_files_to_docs(dir_path=article_directory, clean_func=clean_wiki_text, split_paragraphs=False)
    
    reader = FARMReader(model_name_or_path="deepset/roberta-base-squad2", use_gpu=True)

    prediction = reader.predict(
    query=question,
    documents=docs,
    top_k=1)

    print("Answer:", prediction["answers"][0]["answer"])


    answer = prediction['answers'][0].to_dict()['answer']
    # print(answer)
    return answer



def answerYesNoQuestion(question,file):
    print(question)
    print(file)

    logging.basicConfig(format="%(levelname)s - %(name)s -  %(message)s", level=logging.WARNING)
    logging.getLogger("haystack").setLevel(logging.INFO)


    #### set document folder
    os.mkdir('article_directory')

    # add the argv[1] to the article_directory
    os.system('cp ' + file + ' article_directory')
 

    docs = convert_files_to_docs(dir_path=article_directory, clean_func=clean_wiki_text, split_paragraphs=True)

    print(docs)

    reader = FARMReader(model_name_or_path = "deepset/roberta-base-squad2", use_gpu=True)
    
    prediction = reader.predict(
    query=question,
    documents=docs,
    top_k=1)

    nlp = spacy.load('en_core_web_sm')
    queryTokens = nlp(question)

    ## get 1st predict score and answer
    score = prediction['answers'][0].to_dict()['score']
    answer = prediction['answers'][0].to_dict()['answer']
    answerTokens = nlp(answer)
    sentence_tokens = [[token.text for token in sent] for sent in answerTokens.sents]

    ### add sentence semantic negation
    negateTokens = Negation(sentence_tokens)
    answerNegateSentence = ' '.join(map(str, negateTokens))
    answerNegate = nlp(answerNegateSentence)

    QAsimilarityNegate = queryTokens.similarity(answerNegate)

    QAsimilarity = queryTokens.similarity(answerTokens)  # need to test the QAsimilarity constrain, current useless

    if ((score >= 0.5) ==True) & ((QAsimilarityNegate >0.15) == True):  ## 0.15 from experiment
        answerYN = "Yes"
    else:
        answerYN = "No"

    return answerYN

if __name__ == "__main__":

    # ignore sterr and stdout
    if DEBUG == False:
        sys.stderr = open('/dev/null', 'w')

    try:
        txt_input_file = sys.argv[1]
        print('try')
        print(txt_input_file)
        
        with open(txt_input_file, 'r') as filePath:
            files = filePath.readlines()
        # print(files)   # here assume we are given filePath,  ask do we need to generate .txt files from these path.

        print('trying to test_qs')
        questionFile = sys.argv[2]
        print(questionFile)

 # example  we need to ask if  questions in questions.txt from one article.txt
        with open(questionFile, 'r') as f:
            questions = f.readlines()
            print(questions)
            cnt = 1
            answers = []
            for i in range(len(questions)):
                print(questions[i])
                if isYesNoQuestion(questions[i]) == True:
                    print('returned true')
                    answer = answerYesNoQuestion(questions[i],txt_input_file)
                    print(answer)
                    answers.append(answer)
                else:
                    answer = answerWHQuestion(questions[i],txt_input_file)  ### need to edit file folder here, file is a folder contains .txt files
                    answers.append(answer)

    ############## get the output format #################
            print("***************************")
            # print("A1 ", answers[1])
            # print("A2 ", answers[2])
            # print("A3 ", answers[3])
            for i in range(len(answers)): # ask professor how many questions will be asked ?
                print("A{}".format(i), answers[i])
    except:
        print("Error: This script requires two arguments: (1) textfile.txt (2) number of questions to generate")



#### code for test the system: python3 QG/answer.py QG/data/pikachu_pokemon.txt test_questions.txt
